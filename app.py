import streamlit as st
import os
import pickle
import torch
from PIL import Image
from transformers import pipeline
import numpy as np
import datetime

# Configuraci√≥n de la p√°gina
st.set_page_config(page_title="ü§ñ AI Transformer PLN", layout="wide")

# Sidebar con informaci√≥n de autores y descripci√≥n
with st.sidebar:
    st.title("üíª Autor")
    st.subheader("Desarrollado por:")
    st.markdown("Juan David Arroyave Ramirez") 
    st.markdown('https://davidarroyave.github.io', unsafe_allow_html=True)
    st.caption("Generative Model")
    st.caption("Creative Text Generator - NLP with Transformers")
    st.markdown("---")
    st.info(
        "Prototipo de aplicaci√≥n para Procesamiento de Lenguaje Natural (PLN) usando GPT-2 (DeepESP/gpt2-spanish) tuneado a 20 epochs para generaci√≥n de texto."
    )
    st.markdown("---")
    current_year = datetime.datetime.now().year
    st.markdown(f"""Modelo generador de texto creativo basado en DeepESP GPT-2 de Hugging Face. ¬©{current_year} Juan David Arroyave Ramirez. Licenciado bajo MIT: uso de software permitido seg√∫n los t√©rminos de la licencia MIT. """)
    
    
# T√≠tulo principal y descripci√≥n
st.title(" ü§ñ Generador de Texto Creativo")
st.markdown(
    """
 Bienvenido al **generador de texto creativo con un modelo preentrenado transfomer de  Hugging Face  y desplegado con Streamlit**. 
- **Pesta√±a Generar Texto**: ingresa un prompt para generar salidas.
- **Pesta√±a M√©tricas**: visualiza curva de p√©rdida.
- **Pesta√±a Informe**: Introducci√≥n, marco te√≥rico, soluci√≥n y conclusiones.
"""
)

# Paths
MODEL_DIR = "models"
HISTORY_PATH = os.path.join(MODEL_DIR, "history.pkl")
OUTPUTS_DIR = "outputs"

# Cargar historial para m√©tricas
history = {}
if os.path.exists(HISTORY_PATH):
    with open(HISTORY_PATH, "rb") as f:
        history = pickle.load(f)
else:
    st.warning("No se encontr√≥ el historial de entrenamiento (history.pkl).")

# Tabs
tab_generate, tab_metrics, tab_report = st.tabs(
    ["üéØ Generar Texto", "üìä M√©tricas del Modelo", "üßæ Informe"]
)

# ---------------------
# Tab 1: Generar Texto
# ---------------------
with tab_generate:
    st.subheader("üéØ Generaci√≥n de Texto Creativo en Espa√±ol")
    prompt = st.text_area(
        "Escribe tu prompt aqu√≠:",
        "La inteligencia artificial es importante porque",
        height=120,
    )
    max_length = st.slider(
        "M√°xima longitud de generaci√≥n", min_value=1, max_value=50, value=10
    )
    num_return_sequences = st.slider(
        "N√∫mero de textos a generar", min_value=1, max_value=5, value=2
    )

    if st.button("Generar texto"):
        st.info("Generando‚Ä¶ esto puede tardar unos segundos.")
        try:
            generator = pipeline(
                "text-generation",
                model=MODEL_DIR,
                tokenizer=MODEL_DIR,
                device=0 if torch.cuda.is_available() else -1,
            )
            
            outputs = generator(
                prompt,
                max_length=max_length,
                num_return_sequences=num_return_sequences,
                temperature=0.6,
                top_k=50,
                top_p=0.90,
                no_repeat_ngram_size=2,
        )
            for i, out in enumerate(outputs, start=1):
                st.markdown(f"**Texto generado #{i}:**")
                st.write(out["generated_text"])
                st.markdown("---")
        except Exception as e:
            st.error(f"Ocurri√≥ un error al generar texto: {e}")

# --------------------------
# Tab 2: M√©tricas del Modelo
# --------------------------
with tab_metrics:
    st.subheader("üìä M√©tricas de Entrenamiento")

    loss_curve_path = os.path.join(OUTPUTS_DIR, "loss_curves.jpg")
    if os.path.exists(loss_curve_path):
        st.image(loss_curve_path, caption="Curvas de P√©rdida", use_container_width=True)
    else:
        st.info("La curva de p√©rdida a√∫n no est√° disponible. Ejecuta graficas.py.")

    if history and "train_loss" in history and len(history["train_loss"]) > 0:
        last_train_loss = history["train_loss"][-1]
        ppl = round(float(np.exp(last_train_loss)), 4)
        st.markdown(f"**Perplexity estimada (√∫ltima √©poca):** {ppl}")
    else:
        st.info(" La curva de p√©rdida obtenida baja consistentemente desde aproximadamente 4.8 hasta cerca de 2.5, lo que indica que el modelo est√° aprendiendo y ajustando sus par√°metros correctamente en los datos de entrenamiento")

# ---------------------
# Tab 3: Informe
# ---------------------
with tab_report:
    st.subheader("üßæ Informe del Proyecto")

    st.markdown("## 1. Introducci√≥n")
    st.markdown(
        """
Los modelos de lenguaje basados en la arquitectura Transformer han revolucionado el campo del procesamiento de lenguaje natural (PLN) al permitir capturar dependencias a largo plazo mediante mecanismos de atenci√≥n. GPT2-Spanish, desarrollado por DeepESP, es un modelo autoregresivo preentrenado espec√≠ficamente en grandes cantidades de texto en espa√±ol, cuya capacidad radica en predecir la siguiente palabra de una secuencia dada, generando as√≠ texto fluido y coherente. Gracias a su entrenamiento masivo, GPT2-Spanish cuenta con embeddings que codifican informaci√≥n sem√°ntica y sint√°ctica propia del idioma, lo que facilita tareas de completado, generaci√≥n creativa y traducci√≥n dentro de su dominio ling√º√≠stico.

El proceso de transfer learning sobre GPT2-Spanish consiste en ajustar finamente sus pesos mediante fine-tuning sobre un corpus reducido y especializado, de manera que el modelo refuerce patrones espec√≠ficos y reduzca comportamientos indeseados (como alucinaciones o cambios de idioma). Para ello, primero se tokeniza cada documento empleando la misma estrategia de segmentaci√≥n de subpalabras utilizada en el preentrenamiento, se indexan las secuencias resultantes y se forman embeddings din√°micos que representan cada token en un espacio continuo de alta dimensi√≥n. Durante el entrenamiento, las capas de atenci√≥n y feed-forward se refinan mediante retropropagaci√≥n, optimizando la funci√≥n de p√©rdida de entrop√≠a cruzada.

Este informe detalla el dise√±o de un prototipo en Streamlit que integra GPT2-Spanish afinado, despliega gr√°ficas de p√©rdida versus √©poca para monitorear el ajuste del modelo y permite a los usuarios generar texto creativo en espa√±ol a partir de un prompt. La curva de p√©rdida obtenida baja consistentemente desde aproximadamente 4.8 hasta cerca de 2.5, lo que indica que el modelo est√° aprendiendo y ajustando sus par√°metros correctamente en los datos de entrenamiento.
"""
    )

    st.markdown("## 2. Marco Te√≥rico")
    st.markdown(
        """
**El Transformer** se basa en bloques de atenci√≥n multi-cabeza (multi-head self-attention) y capas de feed-forward totalmente conectadas. En el mecanismo de atenci√≥n, cada token produce queries, keys y values; la similitud entre queries y keys pondera los values, permitiendo al modelo enfocarse en partes relevantes de la secuencia. El uso de atenci√≥n paralela elimina la necesidad de arquitecturas recurrentes, acelerando el entrenamiento y mejorando la captura de relaciones de largo alcance.

**La tokenizaci√≥n** de GPT2-Spanish se realiza mediante Byte-Pair Encoding (BPE), que divide el texto en subpalabras para balancear vocabulario y cobertura. Cada subpalabra recibe un √≠ndice entero y, al procesar una frase, se produce una secuencia de √≠ndices. La indexaci√≥n y el embedding convierten estos √≠ndices en vectores densos de dimensi√≥n fija; estos vectores representan caracter√≠sticas sem√°nticas y sint√°cticas aprendidas durante el preentrenamiento.

**El fine-tuning** adapta un modelo preentrenado a un dominio espec√≠fico ajustando las capas finales y refinando las atenciones. Se emplea una funci√≥n de p√©rdida de entrop√≠a cruzada que mide la discrepancia entre las predicciones y las etiquetas verdaderas. Para mitigar la sobreajuste, se utilizan t√©cnicas como early stopping (detener entrenamiento cuando la m√©trica de validaci√≥n deja de mejorar), weight decay y schedulers de tasa de aprendizaje (ReduceLROnPlateau). Estas estrategias garantizan que el modelo generalice fuera del conjunto de entrenamiento.
"""
    )

    st.markdown("## 3. Descripci√≥n del Problema")
    st.markdown(
        """
El objetivo del prototipo es proporcionar una herramienta de generaci√≥n de texto en espa√±ol que sea creativa, coherente y con un bajo nivel de alucinaciones. Se utilizo un modelo visto en clase, el GPT2-Spanish preentrenado, aunque especializado en espa√±ol, puede incurrir en salidas err√≥neas cuando los prompts son amplios o cuando el modelo no ha visto suficiente variedad de ejemplos. Durante pruebas iniciales con 25 √©pocas de entrenamiento, la curva de validaci√≥n descend√≠a pero todav√≠a mostraba repuntes y ejemplos alucinatorios en ingl√©s. Esto evidenci√≥ la necesidad de un fine-tuning cuidadoso, con early stopping, reducci√≥n de learning rate y un corpus de entrenamiento m√°s homog√©neo en espa√±ol.
"""
    )

    st.markdown("## 4. Planteamiento de la Soluci√≥n")
    st.markdown(
        """
Para abordar el problema, se dise√±√≥ un pipeline de fine-tuning sobre GPT2-Spanish y un despliegue interactivo en Streamlit. Primero, se cre√≥ un entorno con PyTorch, Transformers y Datasets, y se hizo la particion en 80/20 para train y valid.  Luego, se entren√≥ el modelo durante hasta 50 √©pocas con un EarlyStoppingCallback (paciencia = 3), monitoreando eval_loss y reduciendo la tasa de aprendizaje mediante ReduceLROnPlateau. Almacenamos checkpoints por √©poca y registramos historial de p√©rdida.

Se requirio de otro archivo .py para graficar las curvas de p√©rdida, tambien se desarroll√≥ un dashboard en Streamlit con pesta√±as para generaci√≥n de texto, visualizaci√≥n de m√©tricas e informe t√©cnico, integrando el modelo fine-tuneado y las gr√°ficas generadas para interactuar de una manera amigable con la aplicaci√≥n.

El objetivo es facilitar la portabilidad del mismo a traves de PyInstaller y permitir a usuarios sin conocimientos t√©cnicos generar texto creativo y evaluar el desempe√±o del modelo a traves de una interfaz intuitiva y solo abriendo un ejecutable.
"""
    )

    st.markdown("## 5. Resultados")
    st.markdown(
        """
La gr√°fica Loss vs Epochs muestra un descenso continuo de la p√©rdida de validaci√≥n desde 4.8 hasta cerca de 2.5, indicando que el modelo est√° aprendiendo patrones relevantes en el corpus de entrenamiento. La perplexity estimada en la √∫ltima √©poca es aproximadamente 11.95, lo que sugiere que el modelo tiene una buena capacidad para predecir la siguiente palabra en una secuencia dada. El loss de validaci√≥n (Eval Loss) tambi√©n baja, pero mucho m√°s lento, estabiliz√°ndose alrededor de 5.0 despu√©s de la epoch 10. Este comportamiento es t√≠pico en modelos que generalizan razonablemente bien pero podr√≠an beneficiarse de m√°s datos variados o ajustes adicionales.

Un ejemplo de texto generado tras el fine-tuning es:

- Promt: La inteligencia artificial es importante porque

- Output: La mayor√≠a de los sistemas inform√°ticos son capaces de cambiar el mundo, pero la mayor√≠a no lo son. Algunos han perdido el sentido de las matem√°ticas. Los ordenadores y las m√°quinas, sin embargo, han sido incapaces de comprender el funcionamiento de sus sistemas. A pesar de todo, los ordenadores se han convertido en ordenadores, y los tel√©fonos inteligentes han sobrevivido a la primera prueba de Turing. La idea b√°sica de ordenadores es que la informaci√≥n de verdad est√° cambiando el universo, como un virus o una enfermedad. En las √∫ltimas d√©cadas, las tecnolog√≠as han mejorado las condiciones de vida de otros sistemas, incluyendo los juegos inform√°ticos. Se han hecho avances en la evoluci√≥n y la tecnolog√≠a han evolucionado a lo largo del tiempo. Las tecnolog√≠as modernas han avanzado en los primeros a√±os de evoluci√≥n, evoluci√≥n e ingenier√≠a. El desarrollo y el desarrollo han demostrado que las capacidades de software son mejores que los de ingenier√≠a, ya que han ayudado a los tecn√≥logos a superar los retos y a aumentar las oportunidades. Un modelo de desarrollo avanzado es una teor√≠a de inteligencia avanzada. Y ahora hay muchas m√°s personas que intentan imitar las t√©cnicas de pensamiento. Pero estos sistemas son diferentes. Muchos de ellos han aprendido a utilizar sistemas modernos en el pasado, desde la teor√≠a inform√°tica hasta el dise√±o.‚Äù

Este texto refleja fluidez en espa√±ol y coherencia tem√°tica con el prompt, aunque a√∫n muestra ciertas repeticiones ligeras y expresiones redundantes, aspectos sujetos a refinamiento futuro.
"""
    )

    st.markdown("## 6. Conclusiones")
    st.markdown(
        """
El fine-tuning de GPT2-Spanish con early stopping y schedulers permiti√≥ reducir notablemente las alucinaciones y obtener muestras de texto coherente en espa√±ol. No hay se√±al de overfitting severo, ya que la brecha entre train loss y eval loss no aumenta dr√°sticamente. Sin embargo, el estancamiento de eval loss sugiere que el modelo ha llegado cerca de su l√≠mite de generalizaci√≥n para el dataset actual.

El descenso r√°pido del loss de entrenamiento frente al descenso lento de eval loss es normal, pero si la brecha sigue ampli√°ndose, hay riesgo de sobreajuste.

El prototipo en Streamlit facilita la interacci√≥n de usuarios con el modelo, integrando generaci√≥n de texto, visualizaci√≥n de m√©tricas y documentaci√≥n t√©cnica en un solo dashboard. Esta arquitectura modular permite iterar sobre el corpus, ajustar hiperpar√°metros y desplegar nuevas versiones con m√≠nima intervenci√≥n.

Como trabajo futuro se recomienda enriquecer el dataset de fine-tuning con textos de dominio espec√≠fico, aplicar t√©cnicas de filtrado de datos y explorar arquitecturas de enmascaramiento o prompt engineering avanzado para mejorar a√∫n m√°s la coherencia y reducir repeticiones.
"""
    )

    st.markdown("## 7. Referencias")
    st.markdown(
        """
- A. Vaswani et al., ‚ÄúAttention Is All You Need,‚Äù in Advances in Neural Information Processing Systems, 2017, pp. 5998‚Äì6008.
- A. Radford et al., ‚ÄúLanguage Models are Unsupervised Multitask Learners,‚Äù OpenAI, 2019.
- DeepESP, ‚Äúgpt2-spanish,‚Äù Hugging Face Models, 2025. Available: https://huggingface.co/DeepESP/gpt2-spanish
- T. Wolf et al., ‚ÄúTransformers: State-of-the-Art Natural Language Processing,‚Äù Proceedings of EMNLP, 2020.
"""
    )
